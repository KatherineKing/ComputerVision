{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebc3e0a1",
   "metadata": {},
   "source": [
    "### LULC EuroSat Satellite Imagery - SavingLoading - Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c14b07",
   "metadata": {},
   "source": [
    "The EuroSAT dataset\n",
    "27000 labelled Sentinel-2 satellite images of 10 different land uses: residential, industrial, highway, river, forest, pasture, herbaceous vegetation, annual crop, permanent crop and sea/lake. For a reference, see the following papers:\n",
    "\n",
    "Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. Patrick Helber, Benjamin Bischke, Andreas Dengel, Damian Borth. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2019.\n",
    "Introducing EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification. Patrick Helber, Benjamin Bischke, Andreas Dengel. 2018 IEEE International Geoscience and Remote Sensing Symposium, 2018.\n",
    "\n",
    "Subset (roughly stratified): \n",
    "4000 training images\n",
    "1000 testing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7469c8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2cdce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eurosat_data():\n",
    "    data_dir = 'data/'\n",
    "    x_train = np.load(os.path.join(data_dir, 'x_train.npy'))\n",
    "    y_train = np.load(os.path.join(data_dir, 'y_train.npy'))\n",
    "    x_test  = np.load(os.path.join(data_dir, 'x_test.npy'))\n",
    "    y_test  = np.load(os.path.join(data_dir, 'y_test.npy'))\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_eurosat_data()\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96989de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_model(input_shape):\n",
    "    \"\"\"\n",
    "    This function should build a Sequential model according to the above specification. Ensure the \n",
    "    weights are initialised by providing the input_shape argument in the first layer, given by the\n",
    "    function argument.\n",
    "    Your function should also compile the model with the Adam optimiser, sparse categorical cross\n",
    "    entropy loss function, and a single accuracy metric.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Conv2D(16, (3,3), activation='relu', padding='SAME', input_shape=input_shape, name='conv_1'), \n",
    "        Conv2D(8, (3,3), activation='relu', padding='SAME', name='conv_2'), \n",
    "        MaxPooling2D((8,8), name='pool_1'),\n",
    "        Flatten(name='flatten'),\n",
    "        Dense(32, activation='relu', name='dense_1'),\n",
    "        Dense(10, activation='softmax', name='dense_2')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a67267",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_new_model(x_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c7aa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_accuracy(model, x_test, y_test):\n",
    "    \"\"\"Test model classification accuracy\"\"\"\n",
    "    test_loss, test_acc = model.evaluate(x=x_test, y=y_test, verbose=0)\n",
    "    print('accuracy: {acc:0.3f}'.format(acc=test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707bd0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "get_test_accuracy(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b77adf4",
   "metadata": {},
   "source": [
    "3 Callbacks:\n",
    "- checkpoint_every_epoch: checkpoint that saves the model weights every epoch during training\n",
    "- checkpoint_best_only: checkpoint that saves only the weights with the highest validation accuracy. Use the testing data as the validation data.\n",
    "- early_stopping: early stopping object that ends training if the validation accuracy has not improved in 3 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6074f875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_checkpoint_every_epoch():\n",
    "    \"\"\"\n",
    "    This function should return a ModelCheckpoint object that:\n",
    "    - saves the weights only at the end of every epoch\n",
    "    - saves into a directory called 'checkpoints_every_epoch' inside the current working directory\n",
    "    - generates filenames in that directory like 'checkpoint_XXX' where\n",
    "      XXX is the epoch number formatted to have three digits, e.g. 001, 002, 003, etc.\n",
    "    \"\"\"\n",
    "    checkpoint_every_epoch = ModelCheckpoint('checkpoints_every_epoch/checkpoint_{epoch:03d}', \n",
    "                                 save_weights_only = True, \n",
    "                                 frequency='epoch',\n",
    "                                verbose=1)\n",
    "    return checkpoint_every_epoch\n",
    "\n",
    "\n",
    "def get_checkpoint_best_only():\n",
    "    \"\"\"\n",
    "    This function should return a ModelCheckpoint object that:\n",
    "    - saves only the weights that generate the highest validation (testing) accuracy\n",
    "    - saves into a directory called 'checkpoints_best_only' inside the current working directory\n",
    "    - generates a file called 'checkpoints_best_only/checkpoint' \n",
    "    \"\"\"\n",
    "    checkpoint_best_only = ModelCheckpoint(\n",
    "        filepath = 'checkpoints_best_only/checkpoint',\n",
    "        frequency='epoch',\n",
    "        save_best_only=True,\n",
    "        monitor='val_accuracy',\n",
    "        save_weights_only=True, \n",
    "        verbose=1)\n",
    "    return checkpoint_best_only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9459e275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_early_stopping():\n",
    "    \"\"\"\n",
    "    This function should return an EarlyStopping callback that stops training when\n",
    "    the validation (testing) accuracy has not improved in the last 3 epochs.\n",
    "    HINT: use the EarlyStopping callback with the correct 'monitor' and 'patience'\n",
    "    \"\"\"\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)\n",
    "\n",
    "    return early_stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf0eac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_every_epoch = get_checkpoint_every_epoch()\n",
    "checkpoint_best_only = get_checkpoint_best_only()\n",
    "early_stopping = get_early_stopping()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d97b288",
   "metadata": {},
   "source": [
    "Train model using the callbacks\n",
    "Now, you will train the model using the three callbacks you created. If you created the callbacks correctly, three things should happen:\n",
    "\n",
    "At the end of every epoch, the model weights are saved into a directory called checkpoints_every_epoch\n",
    "At the end of every epoch, the model weights are saved into a directory called checkpoints_best_only only if those weights lead to the highest test accuracy\n",
    "Training stops when the testing accuracy has not improved in three epochs.\n",
    "You should then have two directories:\n",
    "\n",
    "A directory called checkpoints_every_epoch containing filenames that include checkpoint_001, checkpoint_002, etc with the 001, 002 corresponding to the epoch\n",
    "A directory called checkpoints_best_only containing filenames that include checkpoint, which contain only the weights leading to the highest testing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76a5c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [checkpoint_every_epoch, checkpoint_best_only, early_stopping]\n",
    "model.fit(x_train, y_train, epochs=50, validation_data=(x_test, y_test), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef70ea5",
   "metadata": {},
   "source": [
    "Create new instance of model and load on both sets of weights\n",
    "Use the saved weights in a fresh model. You should create two functions, both of which take a freshly instantiated model instance:\n",
    "\n",
    "- model_last_epoch should contain the weights from the latest saved epoch\n",
    "- model_best_epoch should contain the weights from the saved epoch with the highest testing accuracy\n",
    "- use the tf.train.latest_checkpoint function to get the filename of the latest saved checkpoint file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfa77e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_last_epoch(model):\n",
    "    \"\"\"\n",
    "    This function should create a new instance of the CNN you created earlier,\n",
    "    load on the weights from the last training epoch, and return this model.\n",
    "    \"\"\"\n",
    "    model.load_weights(tf.train.latest_checkpoint(checkpoint_dir='checkpoints_every_epoch'))\n",
    "    return model\n",
    "    \n",
    "    \n",
    "def get_model_best_epoch(model):\n",
    "    \"\"\"\n",
    "    This function should create a new instance of the CNN you created earlier, load \n",
    "    on the weights leading to the highest validation accuracy, and return this model.\n",
    "    \"\"\"\n",
    "    model.load_weights('checkpoints_best_only/checkpoint')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b5a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to create two models: one with the weights from the last training\n",
    "# epoch, and one with the weights leading to the highest validation (testing) accuracy.\n",
    "# Verify that the second has a higher validation (testing) accuarcy.\n",
    "\n",
    "model_last_epoch = get_model_last_epoch(get_new_model(x_train[0].shape))\n",
    "model_best_epoch = get_model_best_epoch(get_new_model(x_train[0].shape))\n",
    "print('Model with last epoch weights:')\n",
    "get_test_accuracy(model_last_epoch, x_test, y_test)\n",
    "print('')\n",
    "print('Model with best epoch weights:')\n",
    "get_test_accuracy(model_best_epoch, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32efdad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4401cc1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
